{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n",
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total Tokens: 118684\n",
      "Unique Tokens: 7409\n",
      "Total Sequences: 118633\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "118633/118633 [==============================] - 91s 767us/step - loss: 6.3035 - acc: 0.0591\n",
      "Epoch 2/100\n",
      "118633/118633 [==============================] - 91s 770us/step - loss: 5.8637 - acc: 0.0892\n",
      "Epoch 3/100\n",
      "118633/118633 [==============================] - 99s 832us/step - loss: 5.6564 - acc: 0.1057\n",
      "Epoch 4/100\n",
      "118633/118633 [==============================] - 115s 967us/step - loss: 5.4980 - acc: 0.1243\n",
      "Epoch 5/100\n",
      "118633/118633 [==============================] - 117s 985us/step - loss: 5.3839 - acc: 0.1361\n",
      "Epoch 6/100\n",
      "118633/118633 [==============================] - 102s 860us/step - loss: 5.2963 - acc: 0.1447\n",
      "Epoch 7/100\n",
      "118633/118633 [==============================] - 104s 880us/step - loss: 5.2240 - acc: 0.1499\n",
      "Epoch 8/100\n",
      "118633/118633 [==============================] - 121s 1ms/step - loss: 5.1607 - acc: 0.1541\n",
      "Epoch 9/100\n",
      "118633/118633 [==============================] - 104s 880us/step - loss: 5.1340 - acc: 0.1544\n",
      "Epoch 10/100\n",
      "118633/118633 [==============================] - 141s 1ms/step - loss: 5.1152 - acc: 0.1556\n",
      "Epoch 11/100\n",
      "118633/118633 [==============================] - 150s 1ms/step - loss: 5.0299 - acc: 0.1601\n",
      "Epoch 12/100\n",
      "118633/118633 [==============================] - 150s 1ms/step - loss: 4.9721 - acc: 0.1631\n",
      "Epoch 13/100\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.9145 - acc: 0.1662\n",
      "Epoch 14/100\n",
      "118633/118633 [==============================] - 103s 867us/step - loss: 4.8582 - acc: 0.1686\n",
      "Epoch 15/100\n",
      "118633/118633 [==============================] - 119s 1ms/step - loss: 4.8091 - acc: 0.1711\n",
      "Epoch 16/100\n",
      "118633/118633 [==============================] - 107s 905us/step - loss: 4.7648 - acc: 0.1732\n",
      "Epoch 17/100\n",
      "118633/118633 [==============================] - 109s 922us/step - loss: 4.7288 - acc: 0.1746\n",
      "Epoch 18/100\n",
      "118633/118633 [==============================] - 115s 966us/step - loss: 4.7293 - acc: 0.1756\n",
      "Epoch 19/100\n",
      "118633/118633 [==============================] - 87s 733us/step - loss: 4.6361 - acc: 0.1794\n",
      "Epoch 20/100\n",
      "118633/118633 [==============================] - 99s 836us/step - loss: 4.6248 - acc: 0.1814\n",
      "Epoch 21/100\n",
      "118633/118633 [==============================] - 101s 851us/step - loss: 4.5873 - acc: 0.1843\n",
      "Epoch 22/100\n",
      "118633/118633 [==============================] - 102s 862us/step - loss: 4.5323 - acc: 0.1878\n",
      "Epoch 23/100\n",
      "118633/118633 [==============================] - 91s 765us/step - loss: 4.4734 - acc: 0.1897\n",
      "Epoch 24/100\n",
      "118633/118633 [==============================] - 87s 734us/step - loss: 4.4183 - acc: 0.1916\n",
      "Epoch 25/100\n",
      "118633/118633 [==============================] - 88s 743us/step - loss: 4.3967 - acc: 0.1929\n",
      "Epoch 26/100\n",
      "118633/118633 [==============================] - 95s 805us/step - loss: 4.3459 - acc: 0.1949\n",
      "Epoch 27/100\n",
      "118633/118633 [==============================] - 93s 782us/step - loss: 4.2852 - acc: 0.1982\n",
      "Epoch 28/100\n",
      "118633/118633 [==============================] - 95s 800us/step - loss: 4.2319 - acc: 0.2019\n",
      "Epoch 29/100\n",
      "118633/118633 [==============================] - 92s 778us/step - loss: 4.1786 - acc: 0.2056\n",
      "Epoch 30/100\n",
      "118633/118633 [==============================] - 90s 755us/step - loss: 4.1292 - acc: 0.2094\n",
      "Epoch 31/100\n",
      "118633/118633 [==============================] - 101s 854us/step - loss: 4.0770 - acc: 0.2134\n",
      "Epoch 32/100\n",
      "118633/118633 [==============================] - 90s 760us/step - loss: 4.0286 - acc: 0.2166\n",
      "Epoch 33/100\n",
      "118633/118633 [==============================] - 89s 749us/step - loss: 3.9803 - acc: 0.2207\n",
      "Epoch 34/100\n",
      "118633/118633 [==============================] - 90s 754us/step - loss: 3.9338 - acc: 0.2244\n",
      "Epoch 35/100\n",
      "118633/118633 [==============================] - 91s 767us/step - loss: 3.8856 - acc: 0.2302\n",
      "Epoch 36/100\n",
      "118633/118633 [==============================] - 90s 757us/step - loss: 3.8418 - acc: 0.2337\n",
      "Epoch 37/100\n",
      "118633/118633 [==============================] - 94s 790us/step - loss: 3.7993 - acc: 0.2383\n",
      "Epoch 38/100\n",
      "118633/118633 [==============================] - 94s 793us/step - loss: 3.7570 - acc: 0.2428\n",
      "Epoch 39/100\n",
      "118633/118633 [==============================] - 88s 743us/step - loss: 3.7173 - acc: 0.2470\n",
      "Epoch 40/100\n",
      "118633/118633 [==============================] - 85s 715us/step - loss: 3.6769 - acc: 0.2522\n",
      "Epoch 41/100\n",
      "118633/118633 [==============================] - 83s 699us/step - loss: 3.6396 - acc: 0.2563\n",
      "Epoch 42/100\n",
      "118633/118633 [==============================] - 89s 753us/step - loss: 3.6008 - acc: 0.2604\n",
      "Epoch 43/100\n",
      "118633/118633 [==============================] - 91s 768us/step - loss: 3.5642 - acc: 0.2654\n",
      "Epoch 44/100\n",
      "118633/118633 [==============================] - 100s 840us/step - loss: 3.5303 - acc: 0.2690\n",
      "Epoch 45/100\n",
      "118633/118633 [==============================] - 107s 906us/step - loss: 3.4963 - acc: 0.2736\n",
      "Epoch 46/100\n",
      "118633/118633 [==============================] - 116s 978us/step - loss: 3.4604 - acc: 0.2775\n",
      "Epoch 47/100\n",
      "118633/118633 [==============================] - 108s 907us/step - loss: 3.4288 - acc: 0.2836\n",
      "Epoch 48/100\n",
      "118633/118633 [==============================] - 93s 785us/step - loss: 3.3935 - acc: 0.2878\n",
      "Epoch 49/100\n",
      "118633/118633 [==============================] - 79s 669us/step - loss: 3.3623 - acc: 0.2928\n",
      "Epoch 50/100\n",
      "118633/118633 [==============================] - 80s 676us/step - loss: 3.3308 - acc: 0.2979\n",
      "Epoch 51/100\n",
      "118633/118633 [==============================] - 79s 670us/step - loss: 3.2988 - acc: 0.3007\n",
      "Epoch 52/100\n",
      "118633/118633 [==============================] - 81s 683us/step - loss: 3.2704 - acc: 0.3051\n",
      "Epoch 53/100\n",
      "118633/118633 [==============================] - 79s 670us/step - loss: 3.2400 - acc: 0.3104\n",
      "Epoch 54/100\n",
      "118633/118633 [==============================] - 79s 668us/step - loss: 3.2097 - acc: 0.3156\n",
      "Epoch 55/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 3.1806 - acc: 0.3189\n",
      "Epoch 56/100\n",
      "118633/118633 [==============================] - 79s 664us/step - loss: 3.1508 - acc: 0.3245\n",
      "Epoch 57/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 3.1212 - acc: 0.3284\n",
      "Epoch 58/100\n",
      "118633/118633 [==============================] - 79s 664us/step - loss: 3.0960 - acc: 0.3317\n",
      "Epoch 59/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 3.0652 - acc: 0.3362\n",
      "Epoch 60/100\n",
      "118633/118633 [==============================] - 79s 664us/step - loss: 3.0394 - acc: 0.3404\n",
      "Epoch 61/100\n",
      "118633/118633 [==============================] - 79s 666us/step - loss: 3.0141 - acc: 0.3444\n",
      "Epoch 62/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 2.9862 - acc: 0.3481\n",
      "Epoch 63/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 2.9552 - acc: 0.3533\n",
      "Epoch 64/100\n",
      "118633/118633 [==============================] - 79s 666us/step - loss: 2.9303 - acc: 0.3571\n",
      "Epoch 65/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 2.9078 - acc: 0.3614\n",
      "Epoch 66/100\n",
      "118633/118633 [==============================] - 79s 665us/step - loss: 2.8786 - acc: 0.3666\n",
      "Epoch 67/100\n",
      "118633/118633 [==============================] - 79s 666us/step - loss: 2.8526 - acc: 0.3704\n",
      "Epoch 68/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.8267 - acc: 0.3746\n",
      "Epoch 69/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.8010 - acc: 0.3784\n",
      "Epoch 70/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.7766 - acc: 0.3820\n",
      "Epoch 71/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.7540 - acc: 0.3867\n",
      "Epoch 72/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.7284 - acc: 0.3911\n",
      "Epoch 73/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.7035 - acc: 0.3950\n",
      "Epoch 74/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.6803 - acc: 0.3997\n",
      "Epoch 75/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.6569 - acc: 0.4040\n",
      "Epoch 76/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.6348 - acc: 0.4063\n",
      "Epoch 77/100\n",
      "118633/118633 [==============================] - 78s 657us/step - loss: 2.6099 - acc: 0.4116\n",
      "Epoch 78/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.5875 - acc: 0.4154\n",
      "Epoch 79/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.5656 - acc: 0.4186\n",
      "Epoch 80/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.5439 - acc: 0.4234\n",
      "Epoch 81/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.5200 - acc: 0.4265\n",
      "Epoch 82/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.5003 - acc: 0.4301\n",
      "Epoch 83/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.4804 - acc: 0.4332\n",
      "Epoch 84/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.4584 - acc: 0.4385\n",
      "Epoch 85/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.4369 - acc: 0.4417\n",
      "Epoch 86/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.4164 - acc: 0.4458\n",
      "Epoch 87/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.3958 - acc: 0.4504\n",
      "Epoch 88/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.3727 - acc: 0.4533\n",
      "Epoch 89/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.3500 - acc: 0.4584\n",
      "Epoch 90/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.3331 - acc: 0.4617\n",
      "Epoch 91/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.3129 - acc: 0.4641\n",
      "Epoch 92/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.2913 - acc: 0.4690\n",
      "Epoch 93/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.2731 - acc: 0.4737\n",
      "Epoch 94/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.2539 - acc: 0.4769\n",
      "Epoch 95/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.2333 - acc: 0.4810\n",
      "Epoch 96/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.2176 - acc: 0.4835\n",
      "Epoch 97/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.1985 - acc: 0.4862\n",
      "Epoch 98/100\n",
      "118633/118633 [==============================] - 78s 659us/step - loss: 2.1779 - acc: 0.4911\n",
      "Epoch 99/100\n",
      "118633/118633 [==============================] - 78s 658us/step - loss: 2.1631 - acc: 0.4953\n",
      "Epoch 100/100\n",
      "118633/118633 [==============================] - 78s 657us/step - loss: 2.1377 - acc: 0.4993\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "batch_size=256\n",
    "epochs=100\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# save the model to file\n",
    "model_name = str(batch_size) + '-' + str(epochs) + '.h5'\n",
    "model.save(model_name)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the poor and all the pains and pangs which men experience in bringing up a family and in finding money to buy necessaries for their household borrowing and then repudiating getting how they can and giving the money into the hands of women and slaves to keep the many evils of\n",
      "\n",
      "the influence within him and has already made of all sorts of rhythms the poet favours the gods and phrases mount contests into the distance and therefore when they are born of the soul and that he says is the distinction of the keen interval or rational desires in water\n"
     ]
    }
   ],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model(model_name)\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
