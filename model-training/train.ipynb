{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, LeakyReLU, Dropout, BatchNormalization, Conv2D, MaxPool2D, Flatten, Reshape\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_MODEL_PATH = 'models/english-20-model.h5'\n",
    "ENGLISH_TEXT_PATH = 'data/english/republic_sequences.txt'\n",
    "ENGLISH_TOKENIZER_PATH = 'tokenizers/english-20-tokenizer.pkl'\n",
    "\n",
    "CHINESE_MODEL_PATH = 'models/chinese-20-model.h5'\n",
    "CHINESE_TEXT_PATH = 'data/chinese/cleaned-chinese-education.csv'\n",
    "CHINESE_TOKENIZER_PATH = 'tokenizers/chinese-20-tokenizer.pkl'\n",
    "\n",
    "MALAY_MODEL_PATH = 'models/malay-20-model.h5'\n",
    "MALAY_TEXT_PATH = 'data/malay/cleaned-malay-emotions.csv'\n",
    "MALAY_TOKENIZER_PATH = 'tokenizers/malay-20-tokenizer.pkl'\n",
    "\n",
    "MALAYSIAN_MODEL_PATH = 'models/malaysian-20-model.h5'\n",
    "MALAYSIAN_TEXT_PATH = 'data/malaysian/cleaned-tweet.csv'\n",
    "MALAYSIAN_TOKENIZER_PATH = 'tokenizers/malaysian-20-tokenizer.pkl'\n",
    "\n",
    "description = 'english-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english\n",
    "in_filename = 'data/english/republic_sequences.txt'\n",
    "with open(in_filename) as f:\n",
    "    doc = f.read()\n",
    "lines = doc.split('\\n')\n",
    "lines = [' '.join(l.split(' ')[:20]) for l in lines[:4000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "['该 榜 前 十 位 为 湖 南 大 学 四 川 大 学 吉 林 大 学 重 庆', '这 张 许 可 证 在 留 学 签 证 有 效 期 间 内 都 有 效 如 果', '两 岸 期 盼 和 平 统 一 的 心 情 一 样 的 和 谐 相 同 的 迫', '北 京 成 招 网 上 报 名 正 在 进 行 考 生 报 考 时 要 注 意', '最 先 开 始 的 提 前 批 次 录 取 时 间 月 日 日 参 加 该 批']\n"
     ]
    }
   ],
   "source": [
    "# load chinese/malay/malaysian\n",
    "df = pd.read_csv(CHINESE_TEXT_PATH)\n",
    "lines = list(df['text'])\n",
    "lines = random.sample(lines, 4000)\n",
    "print(len(lines))\n",
    "print(lines[:5])\n",
    "\n",
    "# lines = list(filter(lambda x: len(x.split(' ')) >= 20, lines))\n",
    "# lines = [' '.join(l.split(' ')[-20:]) for l in lines]\n",
    "# df.head()\n",
    "\n",
    "# for l in lines:\n",
    "#     assert len(l.split(' ')) == 20, str(len(l.split(' ')))\n",
    "# lines[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2239"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = pickle.load(open(ENGLISH_TOKENIZER_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "pickle.dump(tokenizer, open(description + '-tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(corpus, tokenizer):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocab_size = get_sequence_of_tokens(lines, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[235, 232, 827, 2, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "def generate_padded_sequences(input_sequences, total_words):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    print(max_sequence_len)\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "X, y, seq_length = generate_padded_sequences(X, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # separate into input and output\n",
    "# sequences = np.array(sequences)\n",
    "# X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(ENGLISH_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 19, 512)           1146880   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 19, 128)           328192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2240)              4589760   \n",
      "=================================================================\n",
      "Total params: 15,131,840\n",
      "Trainable params: 15,123,648\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 512, input_length=seq_length - 1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# model.add(Reshape((10, 10, 1)))\n",
    "\n",
    "# model.add(Conv2D(256, 3, input_shape=(10, 10, 1)))\n",
    "# # model.add(MaxPool2D(2))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(512, 3))\n",
    "# # model.add(MaxPool2D(2))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(4096))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Dense(8096))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2048))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss',\n",
    "                  patience=3,\n",
    "                  verbose=1,\n",
    "                  mode='min',\n",
    "                  restore_best_weights=True)\n",
    "mc = ModelCheckpoint(description + '-best_Weights.h5',\n",
    "                     monitor='loss',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True)\n",
    "# model.load_weights(MALAYSIAN_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "76000/76000 [==============================] - 20s 257us/step - loss: 0.2079 - acc: 0.9428\n",
      "\n",
      "Epoch 00001: loss improved from 0.22080 to 0.20791, saving model to english-20-best_Weights.h5\n",
      "Epoch 2/10\n",
      "76000/76000 [==============================] - 19s 255us/step - loss: 0.2061 - acc: 0.9428\n",
      "\n",
      "Epoch 00002: loss improved from 0.20791 to 0.20610, saving model to english-20-best_Weights.h5\n",
      "Epoch 3/10\n",
      "76000/76000 [==============================] - 19s 255us/step - loss: 0.2047 - acc: 0.9428\n",
      "\n",
      "Epoch 00003: loss improved from 0.20610 to 0.20465, saving model to english-20-best_Weights.h5\n",
      "Epoch 4/10\n",
      "76000/76000 [==============================] - 19s 256us/step - loss: 0.2036 - acc: 0.9423\n",
      "\n",
      "Epoch 00004: loss improved from 0.20465 to 0.20361, saving model to english-20-best_Weights.h5\n",
      "Epoch 5/10\n",
      "76000/76000 [==============================] - 19s 256us/step - loss: 0.2036 - acc: 0.9419\n",
      "\n",
      "Epoch 00005: loss improved from 0.20361 to 0.20359, saving model to english-20-best_Weights.h5\n",
      "Epoch 6/10\n",
      "76000/76000 [==============================] - 20s 261us/step - loss: 0.2018 - acc: 0.9418\n",
      "\n",
      "Epoch 00006: loss improved from 0.20359 to 0.20184, saving model to english-20-best_Weights.h5\n",
      "Epoch 7/10\n",
      "76000/76000 [==============================] - 19s 256us/step - loss: 0.2023 - acc: 0.9427\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.20184\n",
      "Epoch 8/10\n",
      "76000/76000 [==============================] - 19s 257us/step - loss: 0.2003 - acc: 0.9421\n",
      "\n",
      "Epoch 00008: loss improved from 0.20184 to 0.20028, saving model to english-20-best_Weights.h5\n",
      "Epoch 9/10\n",
      "76000/76000 [==============================] - 20s 259us/step - loss: 0.2010 - acc: 0.9419\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.20028\n",
      "Epoch 10/10\n",
      "76000/76000 [==============================] - 20s 257us/step - loss: 0.2009 - acc: 0.9419\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.20028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1aa0bd7048>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "batch_size=1024\n",
    "epochs=10\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "weights_name = description + '-weights.h5'\n",
    "model.save_weights(weights_name)\n",
    "\n",
    "model_name = description + '-model.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    # encode the text as integer\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    # truncate sequences to a fixed length\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # predict probabilities for each word\n",
    "    # yhat = self.model.predict_classes(encoded, verbose=0)\n",
    "    predicted_l = list(tuple(enumerate(model.predict(encoded)[0])))\n",
    "    top_3 = sorted(predicted_l, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(top_3)\n",
    "    # map predicted word index to word\n",
    "    predicted_words = []\n",
    "    for i, word in enumerate(top_3):\n",
    "        for w in list(tokenizer.word_index.items()):\n",
    "            if w[1] == word[0]:\n",
    "                predicted_words.append({'word': w[0], 'probability': word[1]})\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, 0.9998729), (4, 8.585863e-05), (204, 4.0247425e-05)]\n",
      "[{'word': 'for', 'probability': 0.9998729}, {'word': 'of', 'probability': 8.585863e-05}, {'word': 'whose', 'probability': 4.0247425e-05}]\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, tokenizer, 19, 'my father and grandfather', 1)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
